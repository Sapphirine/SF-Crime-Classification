{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV  \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def parse_time(x):\n",
    "    DD = datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\n",
    "#     year = DD.year-2002\n",
    "    time = DD.hour\n",
    "    day = DD.day\n",
    "    month = DD.month\n",
    "    return time,day,month\n",
    "\n",
    "def odds(x):\n",
    "    if x == 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return np.log(x)-np.log(1-x)\n",
    "def update_odds(x, default_logodds, oddUpdate):\n",
    "    defaut = default_logodds\n",
    "    val = oddUpdate[x]\n",
    "    if len(val)!=1:\n",
    "        defaut[val.keys()] = val\n",
    "    return pd.Series(defaut)\n",
    "\n",
    "rough_category = dict()\n",
    "rough_category['VEHICLE THEFT'] = 15\n",
    "rough_category['VANDALISM'] = 0\n",
    "rough_category['DRIVING UNDER THE INFLUENCE'] = 15\n",
    "rough_category['ARSON'] = 19\n",
    "rough_category['BRIBERY'] = 8\n",
    "rough_category['SUICIDE'] = 9\n",
    "rough_category['SEX OFFENSES NON FORCIBLE'] = 0\n",
    "rough_category['EXTORTION'] = 20\n",
    "rough_category['GAMBLING'] = 0\n",
    "rough_category['BAD CHECKS'] = 23\n",
    "rough_category['TREA'] = 0\n",
    "rough_category['RECOVERED VEHICLE'] = 19\n",
    "rough_category['PORNOGRAPHY/OBSCENE MAT'] = 0\n",
    "rough_category['WARRANTS'] = 1\n",
    "rough_category['OTHER OFFENSES'] = 1\n",
    "rough_category['LARCENY/THEFT'] = 1\n",
    "rough_category['NON-CRIMINAL'] = 1\n",
    "rough_category['ROBBERY'] = 1\n",
    "rough_category['ASSAULT'] = 1\n",
    "rough_category['WEAPON LAWS'] = 11\n",
    "rough_category['DRUNKENNESS'] = 14\n",
    "rough_category['TRESPASS'] = 1\n",
    "rough_category['LOITERING'] = 10\n",
    "rough_category['BURGLARY'] = 2\n",
    "rough_category['SECONDARY CODES'] = 16\n",
    "rough_category['MISSING PERSON'] = 12\n",
    "rough_category['RUNAWAY'] = 13\n",
    "rough_category['FAMILY OFFENSES'] = 22\n",
    "rough_category['LIQUOR LAWS'] = 14\n",
    "rough_category['DISORDERLY CONDUCT'] = 21\n",
    "rough_category['SUSPICIOUS OCC'] = 17\n",
    "rough_category['KIDNAPPING'] = 16\n",
    "rough_category['SEX OFFENSES FORCIBLE'] = 17\n",
    "rough_category['EMBEZZLEMENT'] = 18\n",
    "rough_category['DRUG/NARCOTIC'] = 3\n",
    "rough_category['PROSTITUTION'] = 4\n",
    "rough_category['FORGERY/COUNTERFEITING'] = 5\n",
    "rough_category['FRAUD'] = 6\n",
    "rough_category['STOLEN PROPERTY'] = 7\n",
    "\n",
    "def experiment(file_name):\n",
    "    df = pd.read_csv(file_name) \n",
    "    to_learn = pd.DataFrame()\n",
    "    to_learn['Category'] = df.Category.apply(lambda item: rough_category[item])\n",
    "    df['NewCategory'] = to_learn['Category']\n",
    "    to_learn['Hour'], to_learn['Day'], to_learn['Month'] = zip(*df.Dates.apply(parse_time))\n",
    "    to_learn['X'], to_learn['Y'] = df.X, df.Y\n",
    "    SFPD = df.PdDistrict.unique()\n",
    "    PD_map, label = dict(), 1\n",
    "    for name in SFPD:\n",
    "        PD_map[name] = label\n",
    "        label += 1\n",
    "    to_learn['PD'] = df.PdDistrict.apply(lambda item: PD_map[item])\n",
    "\n",
    "    #to_learn.to_csv('cat_feature_' + str(file_no) + '.csv')\n",
    "    #to_learn = pd.read_csv('cat_feature.csv')\n",
    "    to_learn.X, to_learn.Y = 10 * scale(to_learn.X), 10 * scale(to_learn.Y)\n",
    "    \n",
    "    \n",
    "    addresses = sorted(df[\"Address\"].unique())\n",
    "    categories = sorted(df[\"NewCategory\"].unique())\n",
    "    C_counts = df.groupby([\"NewCategory\"]).size()\n",
    "    logoddsPA2 = dict((df.groupby('Address').size()/len(df)).apply(odds))\n",
    "    default_logodds = np.log(C_counts/len(df))-np.log(1.0-C_counts/float(len(df)))\n",
    "    oddUpdate = pd.Series(((df.groupby(['Address','NewCategory']).size()/df.groupby(['Address']).size()).apply(odds)))\n",
    "    logodds2 = {k:update_odds(k, default_logodds, oddUpdate) for k in addresses}\n",
    "    address_features=df[\"Address\"].apply(lambda x: logodds2[x])\n",
    "    address_features.columns=[\"logodds\"+str(x) for x in range(len(address_features.columns))]\n",
    "\n",
    "    to_learn = pd.concat([to_learn, address_features], axis=1)\n",
    "    \n",
    "    col = [col for col in to_learn.columns if col not in ['Category']]\n",
    "    category, features = to_learn.Category.as_matrix(), to_learn[col].as_matrix()\n",
    "\n",
    "    new_PCA=PCA(n_components=15)\n",
    "    new_PCA.fit(features)\n",
    "    return new_PCA.explained_variance_ratio_\n",
    "#     new_PCA=PCA(n_components=8)\n",
    "#     features = new_PCA.fit_transform(features)\n",
    "#     print(file_name)\n",
    "#     average = []\n",
    "#     features_train, features_test, category_train, category_test = train_test_split(features, category, test_size=0.2)\n",
    "#     model      = GradientBoostingClassifier(max_features='log2',n_estimators = 100)\n",
    "#     grid_model = model.fit(features_train, category_train)\n",
    "#     prediction = grid_model.predict(features_test)\n",
    "#     print accuracy_score(category_test, prediction)\n",
    "#     average.append(accuracy_score(category_test, prediction))\n",
    "#     return average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003.csv\n",
      "0.603003856302\n",
      "0.603815709357\n",
      "0.607401393681\n",
      "0.602124348826\n",
      "0.606048305257\n",
      "0.608280901157\n",
      "0.610445842636\n",
      "0.600568297138\n",
      "0.60455990799\n",
      "0.606927812733\n",
      "Average:      0.605317637508\n",
      "2004.csv\n",
      "0.588968335036\n",
      "0.586789240722\n",
      "0.584678243105\n",
      "0.584473953013\n",
      "0.586176370446\n",
      "0.58658495063\n",
      "0.588764044944\n",
      "0.58937691522\n",
      "0.587538304392\n",
      "0.591828396323\n",
      "Average:      0.587517875383\n",
      "2005.csv\n",
      "0.576080813789\n",
      "0.570570782707\n",
      "0.576787228031\n",
      "0.574526702458\n",
      "0.580319299237\n",
      "0.578765187906\n",
      "0.580319299237\n",
      "0.585264198926\n",
      "0.581661486296\n",
      "0.573396439672\n",
      "Average:      0.577769143826\n",
      "2006.csv\n",
      "0.623444428551\n",
      "0.609712487484\n",
      "0.616149334859\n",
      "0.619153196968\n",
      "0.617508224861\n",
      "0.615791732227\n",
      "0.617436704334\n",
      "0.612859390645\n",
      "0.617150622229\n",
      "0.620583607495\n",
      "Average:      0.616978972965\n",
      "2007.csv\n",
      "0.621921634933\n",
      "0.618098948761\n",
      "0.616555171653\n",
      "0.623171359259\n",
      "0.618245975153\n",
      "0.616628684849\n",
      "0.617510843196\n",
      "0.619201646696\n",
      "0.618245975153\n",
      "0.619642725869\n",
      "Average:      0.618922296552\n",
      "2008.csv\n",
      "0.629711435696\n",
      "0.626433915212\n",
      "0.626790167439\n",
      "0.625222657642\n",
      "0.625151407196\n",
      "0.622728892056\n",
      "0.622586391165\n",
      "0.620092625579\n",
      "0.623370146063\n",
      "0.628713929462\n",
      "Average:      0.625080156751\n",
      "2009.csv\n",
      "0.622463768116\n",
      "0.629565217391\n",
      "0.626739130435\n",
      "0.622898550725\n",
      "0.62384057971\n",
      "0.624927536232\n",
      "0.630724637681\n",
      "0.627391304348\n",
      "0.627898550725\n",
      "0.626304347826\n",
      "Average:      0.626275362319\n",
      "2010.csv\n",
      "0.627470132993\n",
      "0.63100157788\n",
      "0.636110902397\n",
      "0.634908708393\n",
      "0.632278909009\n",
      "0.633105417387\n",
      "0.631903223383\n",
      "0.638966113157\n",
      "0.640093170035\n",
      "0.636110902397\n",
      "Average:      0.634194905703\n",
      "2011.csv\n",
      "0.651681176824\n",
      "0.656259381567\n",
      "0.653182227559\n",
      "0.656784749325\n",
      "0.649954968478\n",
      "0.650255178625\n",
      "0.648228760132\n",
      "0.649129390573\n",
      "0.656709696788\n",
      "0.6489792855\n",
      "Average:      0.652116481537\n",
      "2012.csv\n",
      "0.653934620478\n",
      "0.65323761065\n",
      "0.657837875514\n",
      "0.656513556841\n",
      "0.658395483376\n",
      "0.663413954137\n",
      "0.658116679445\n",
      "0.653655816547\n",
      "0.650449571339\n",
      "0.652122394926\n",
      "Average:      0.655767756325\n",
      "2013.csv\n",
      "0.688070361063\n",
      "0.682383282635\n",
      "0.681259092713\n",
      "0.684962306573\n",
      "0.690054225632\n",
      "0.683110699643\n",
      "0.684102631927\n",
      "0.689128422166\n",
      "0.693625181854\n",
      "0.691178415553\n",
      "Average:      0.686787461976\n",
      "2014.csv\n",
      "0.691921893808\n",
      "0.696001069948\n",
      "0.694529891668\n",
      "0.692189380768\n",
      "0.697137889528\n",
      "0.688578306808\n",
      "0.698475324328\n",
      "0.693928046008\n",
      "0.691587535108\n",
      "0.695131737328\n",
      "Average:      0.69394810753\n",
      "Average for 2003-2014:0.631723013198\n"
     ]
    }
   ],
   "source": [
    "All_average = []\n",
    "for i in range(2003, 2015):\n",
    "    All_average.append(experiment(str(i) + '.csv'))\n",
    "print 'Average for 2003-2014:' + str(np.average(All_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003.csv\n",
      "0.599959407347\n",
      "Average:            0.599959407347\n",
      "2004.csv\n",
      "0.574736125298\n",
      "Average:            0.574736125298\n",
      "2005.csv\n",
      "0.559056230574\n",
      "Average:            0.559056230574\n",
      "2006.csv\n",
      "0.607495351166\n",
      "Average:            0.607495351166\n",
      "2007.csv\n",
      "0.614643828567\n",
      "Average:            0.614643828567\n",
      "2008.csv\n",
      "0.612896330602\n",
      "Average:            0.612896330602\n",
      "2009.csv\n",
      "0.613333333333\n",
      "Average:            0.613333333333\n",
      "2010.csv\n",
      "0.618829363589\n",
      "Average:            0.618829363589\n",
      "2011.csv\n",
      "0.643350345242\n",
      "Average:            0.643350345242\n",
      "2012.csv\n",
      "0.648010036942\n",
      "Average:            0.648010036942\n",
      "2013.csv\n",
      "0.679076841688\n",
      "Average:            0.679076841688\n",
      "2014.csv\n",
      "0.684164771967\n",
      "Average:            0.684164771967\n",
      "Average for 2003-2014:0.621295997193\n"
     ]
    }
   ],
   "source": [
    "All_average = []\n",
    "for i in range(2003, 2015):\n",
    "    All_average.append(experiment(str(i) + '.csv'))\n",
    "print 'Average for 2003-2014:'.ljust(20) + str(np.average(All_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003.csv\n",
      "0.585278397943\n",
      "2004.csv\n",
      "0.570241743275\n",
      "2005.csv\n",
      "0.557714043515\n",
      "2006.csv\n",
      "0.602560434845\n",
      "2007.csv\n",
      "0.598617951922\n",
      "2008.csv\n",
      "0.600356252227\n",
      "2009.csv\n",
      "0.610217391304\n",
      "2010.csv\n",
      "0.61642497558\n",
      "2011.csv\n",
      "0.639072350645\n",
      "2012.csv\n",
      "0.635603262006\n",
      "2013.csv\n",
      "0.667901071287\n",
      "2014.csv\n",
      "0.679884980607\n",
      "Average for 2003-2014:0.613656071263\n"
     ]
    }
   ],
   "source": [
    "All_average = []\n",
    "for i in range(2003, 2015):\n",
    "    All_average.append(experiment(str(i) + '.csv'))\n",
    "print 'Average for 2003-2014:'.ljust(20) + str(np.average(All_average))\n",
    "# 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003.csv\n",
      "0.593261619647\n",
      "2004.csv\n",
      "0.573987061628\n",
      "2005.csv\n",
      "0.553616840916\n",
      "2006.csv\n",
      "0.59612358747\n",
      "2007.csv\n",
      "0.596706608836\n",
      "2008.csv\n",
      "0.607837548985\n",
      "2009.csv\n",
      "0.606666666667\n",
      "2010.csv\n",
      "0.614546547449\n",
      "2011.csv\n",
      "0.635544881417\n",
      "2012.csv\n",
      "0.637624590507\n",
      "2013.csv\n",
      "0.662809152229\n",
      "2014.csv\n",
      "0.681222415407\n",
      "Average for 2003-2014:0.613328960096\n"
     ]
    }
   ],
   "source": [
    "All_average = []\n",
    "for i in range(2003, 2015):\n",
    "    All_average.append(experiment(str(i) + '.csv'))\n",
    "print 'Average for 2003-2014:'.ljust(20) + str(np.average(All_average))\n",
    "# 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003.csv\n",
      "0.585481361207\n",
      "2004.csv\n",
      "0.56758597208\n",
      "2005.csv\n",
      "0.547753602713\n",
      "2006.csv\n",
      "0.594264053783\n",
      "2007.csv\n",
      "0.60486657355\n",
      "2008.csv\n",
      "0.606056287852\n",
      "2009.csv\n",
      "0.599492753623\n",
      "2010.csv\n",
      "0.606206326546\n",
      "2011.csv\n",
      "0.630066046232\n",
      "2012.csv\n",
      "0.634139541368\n",
      "2013.csv\n",
      "0.664859145616\n",
      "2014.csv\n",
      "0.673532165307\n",
      "Average for 2003-2014:0.609525319156\n"
     ]
    }
   ],
   "source": [
    "#24\n",
    "All_average = []\n",
    "for i in range(2003, 2015):\n",
    "    All_average.append(experiment(str(i) + '.csv'))\n",
    "print 'Average for 2003-2014:'.ljust(20) + str(np.average(All_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(1)\n",
    "for i in range(2003, 2015):\n",
    "    plt.subplot(3, 4, i - 2002)\n",
    "    plt.plot(experiment(str(i) + '.csv'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try All Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV  \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def parse_time(x):\n",
    "    DD = datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\n",
    "#     year = DD.year-2002\n",
    "    time = DD.hour\n",
    "    day = DD.day\n",
    "    month = DD.month\n",
    "    return time,day,month\n",
    "\n",
    "def odds(x):\n",
    "    if x == 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return np.log(x)-np.log(1-x)\n",
    "def update_odds(x, default_logodds, oddUpdate):\n",
    "    defaut = default_logodds\n",
    "    val = oddUpdate[x]\n",
    "    if len(val)!=1:\n",
    "        defaut[val.keys()] = val\n",
    "    return pd.Series(defaut)\n",
    "\n",
    "rough_category = dict()\n",
    "rough_category['VEHICLE THEFT'] = 0\n",
    "rough_category['VANDALISM'] = 1\n",
    "rough_category['DRIVING UNDER THE INFLUENCE'] = 0\n",
    "rough_category['ARSON'] = 2\n",
    "rough_category['BRIBERY'] = 3\n",
    "rough_category['SUICIDE'] = 4\n",
    "rough_category['SEX OFFENSES NON FORCIBLE'] = 5\n",
    "rough_category['EXTORTION'] = 6\n",
    "rough_category['GAMBLING'] = 7\n",
    "rough_category['BAD CHECKS'] = 8\n",
    "rough_category['TREA'] = 9\n",
    "rough_category['RECOVERED VEHICLE'] = 10\n",
    "rough_category['PORNOGRAPHY/OBSCENE MAT'] = 11\n",
    "rough_category['WARRANTS'] = 12\n",
    "rough_category['OTHER OFFENSES'] = 13\n",
    "rough_category['LARCENY/THEFT'] = 14\n",
    "rough_category['NON-CRIMINAL'] = 15\n",
    "rough_category['ROBBERY'] = 16\n",
    "rough_category['ASSAULT'] = 17\n",
    "rough_category['WEAPON LAWS'] = 18\n",
    "rough_category['DRUNKENNESS'] = 19\n",
    "rough_category['TRESPASS'] = 9\n",
    "rough_category['LOITERING'] = 20\n",
    "rough_category['BURGLARY'] = 21\n",
    "rough_category['SECONDARY CODES'] = 22\n",
    "rough_category['MISSING PERSON'] = 23\n",
    "rough_category['RUNAWAY'] = 24\n",
    "rough_category['FAMILY OFFENSES'] = 25\n",
    "rough_category['LIQUOR LAWS'] = 19\n",
    "rough_category['DISORDERLY CONDUCT'] = 26\n",
    "rough_category['SUSPICIOUS OCC'] = 27\n",
    "rough_category['KIDNAPPING'] = 28\n",
    "rough_category['SEX OFFENSES FORCIBLE'] = 29\n",
    "rough_category['EMBEZZLEMENT'] = 30\n",
    "rough_category['DRUG/NARCOTIC'] = 31\n",
    "rough_category['PROSTITUTION'] = 32\n",
    "rough_category['FORGERY/COUNTERFEITING'] = 33\n",
    "rough_category['FRAUD'] = 34\n",
    "rough_category['STOLEN PROPERTY'] = 35\n",
    "\n",
    "def experiment(file_name):\n",
    "    df = pd.read_csv(file_name) \n",
    "    to_learn = pd.DataFrame()\n",
    "    to_learn['Category'] = df.Category.apply(lambda item: rough_category[item])\n",
    "    df['NewCategory'] = to_learn['Category']\n",
    "    to_learn['Hour'], to_learn['Day'], to_learn['Month'] = zip(*df.Dates.apply(parse_time))\n",
    "    to_learn['X'], to_learn['Y'] = df.X, df.Y\n",
    "    SFPD = df.PdDistrict.unique()\n",
    "    PD_map, label = dict(), 1\n",
    "    for name in SFPD:\n",
    "        PD_map[name] = label\n",
    "        label += 1\n",
    "    to_learn['PD'] = df.PdDistrict.apply(lambda item: PD_map[item])\n",
    "\n",
    "    #to_learn.to_csv('cat_feature_' + str(file_no) + '.csv')\n",
    "    #to_learn = pd.read_csv('cat_feature.csv')\n",
    "    to_learn.X, to_learn.Y = 10 * scale(to_learn.X), 10 * scale(to_learn.Y)\n",
    "    \n",
    "    \n",
    "    addresses = sorted(df[\"Address\"].unique())\n",
    "    categories = sorted(df[\"NewCategory\"].unique())\n",
    "    C_counts = df.groupby([\"NewCategory\"]).size()\n",
    "    logoddsPA2 = dict((df.groupby('Address').size()/len(df)).apply(odds))\n",
    "    default_logodds = np.log(C_counts/len(df))-np.log(1.0-C_counts/float(len(df)))\n",
    "    oddUpdate = pd.Series(((df.groupby(['Address','NewCategory']).size()/df.groupby(['Address']).size()).apply(odds)))\n",
    "    logodds2 = {k:update_odds(k, default_logodds, oddUpdate) for k in addresses}\n",
    "    address_features=df[\"Address\"].apply(lambda x: logodds2[x])\n",
    "    address_features.columns=[\"logodds\"+str(x) for x in range(len(address_features.columns))]\n",
    "\n",
    "    to_learn = pd.concat([to_learn, address_features], axis=1)\n",
    "    \n",
    "    col = [col for col in to_learn.columns if col not in ['Category']]\n",
    "    category, features = to_learn.Category.as_matrix(), to_learn[col].as_matrix()\n",
    "\n",
    "#     new_PCA=PCA(n_components=15)\n",
    "#     new_PCA.fit(features)\n",
    "#     return new_PCA.explained_variance_ratio_\n",
    "    new_PCA=PCA(n_components=7)\n",
    "    features = new_PCA.fit_transform(features)\n",
    "    print(file_name)\n",
    "    average = []\n",
    "#     for i in range(0,10):\n",
    "#         features_train, features_test, category_train, category_test = train_test_split(features, category, test_size=0.2)\n",
    "#         model      = GradientBoostingClassifier(max_features='log2',n_estimators = 40)\n",
    "#         grid_model = model.fit(features_train, category_train)\n",
    "#         prediction = grid_model.predict(features_test)\n",
    "#         print accuracy_score(category_test, prediction)\n",
    "#         average.append(accuracy_score(category_test, prediction))\n",
    "    features_train, features_test, category_train, category_test = train_test_split(features, category, test_size=0.1)\n",
    "    model      = GradientBoostingClassifier(max_features='log2',n_estimators = 20)\n",
    "    grid_model = model.fit(features_train, category_train)\n",
    "    prediction = grid_model.predict(features_test)\n",
    "    print accuracy_score(category_test, prediction)\n",
    "    average.append(accuracy_score(category_test, prediction))\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
