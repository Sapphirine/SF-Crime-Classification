{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Year Split\n",
    "This block serves as splitting the original data by year and save resulting sub datasets to .csv files. Note here data in 2015 are discarded because the sample space is too small comparing to other years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "def parse_time(x):\n",
    "    DD = datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\n",
    "    return DD.year\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df['Year'] = df['Dates'].apply(parse_time)\n",
    "\n",
    "years = list()\n",
    "for i in range(2003, 2016):\n",
    "    years.append(df[df.Year == i])\n",
    "    \n",
    "for i in range(2003, 2015):\n",
    "    years[i-2003].to_csv(str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "This block include in all dependencies, including\n",
    "* External libraries like numpy and sklearn;\n",
    "* Custom functions served for parsing timestamp, and calculating counting-based log odds;\n",
    "* Merge orignal categories using new label set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def parse_time(x):\n",
    "    DD = datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\n",
    "    time = DD.hour\n",
    "    day = DD.day\n",
    "    month = DD.month\n",
    "    return time,day,month\n",
    "\n",
    "def odds(x):\n",
    "    if x == 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return np.log(x)-np.log(1-x)\n",
    "    \n",
    "def update_odds(x, default_logodds, oddUpdate):\n",
    "    defaut = default_logodds\n",
    "    val = oddUpdate[x]\n",
    "    if len(val)!=1:\n",
    "        defaut[val.keys()] = val\n",
    "    return pd.Series(defaut)\n",
    "\n",
    "rough_category = dict()\n",
    "rough_category['VEHICLE THEFT'] = 0\n",
    "rough_category['VANDALISM'] = 0\n",
    "rough_category['DRIVING UNDER THE INFLUENCE'] = 0\n",
    "rough_category['ARSON'] = 0\n",
    "rough_category['BRIBERY'] = 0\n",
    "rough_category['SUICIDE'] = 0\n",
    "rough_category['SEX OFFENSES NON FORCIBLE'] = 0\n",
    "rough_category['EXTORTION'] = 0\n",
    "rough_category['GAMBLING'] = 0\n",
    "rough_category['BAD CHECKS'] = 0\n",
    "rough_category['TREA'] = 0\n",
    "rough_category['RECOVERED VEHICLE'] = 0\n",
    "rough_category['PORNOGRAPHY/OBSCENE MAT'] = 0\n",
    "rough_category['WARRANTS'] = 1\n",
    "rough_category['OTHER OFFENSES'] = 1\n",
    "rough_category['LARCENY/THEFT'] = 1\n",
    "rough_category['NON-CRIMINAL'] = 1\n",
    "rough_category['ROBBERY'] = 1\n",
    "rough_category['ASSAULT'] = 1\n",
    "rough_category['WEAPON LAWS'] = 1\n",
    "rough_category['DRUNKENNESS'] = 1\n",
    "rough_category['TRESPASS'] = 1\n",
    "rough_category['LOITERING'] = 1\n",
    "rough_category['BURGLARY'] = 2\n",
    "rough_category['SECONDARY CODES'] = 2\n",
    "rough_category['MISSING PERSON'] = 2\n",
    "rough_category['RUNAWAY'] = 2\n",
    "rough_category['FAMILY OFFENSES'] = 2\n",
    "rough_category['LIQUOR LAWS'] = 2\n",
    "rough_category['DISORDERLY CONDUCT'] = 2\n",
    "rough_category['SUSPICIOUS OCC'] = 2\n",
    "rough_category['KIDNAPPING'] = 2\n",
    "rough_category['SEX OFFENSES FORCIBLE'] = 2\n",
    "rough_category['EMBEZZLEMENT'] = 2\n",
    "rough_category['DRUG/NARCOTIC'] = 3\n",
    "rough_category['PROSTITUTION'] = 4\n",
    "rough_category['FORGERY/COUNTERFEITING'] = 5\n",
    "rough_category['FRAUD'] = 5\n",
    "rough_category['STOLEN PROPERTY'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Experiment Function\n",
    "This block is the main function for feature extraction and machine learning. It is roughly composed of four parts:\n",
    "* Extract temporal and geographical features;\n",
    "* Extract count-based log odd ratio features;\n",
    "* Conduct PCA on data sets for dimension reudction;\n",
    "* Machine learning using random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def experiment(file_name, csv_num):\n",
    "    df = pd.read_csv(file_name) \n",
    "    to_learn = pd.DataFrame()\n",
    "    to_learn['Category'] = df.Category.apply(lambda item: rough_category[item])\n",
    "    df['NewCategory'] = to_learn['Category']\n",
    "    to_learn['Hour'], to_learn['Day'], to_learn['Month'] = zip(*df.Dates.apply(parse_time))\n",
    "    to_learn['X'], to_learn['Y'] = df.X, df.Y\n",
    "    SFPD = df.PdDistrict.unique()\n",
    "    PD_map, label = dict(), 1\n",
    "    for name in SFPD:\n",
    "        PD_map[name] = label\n",
    "        label += 1\n",
    "    to_learn['PD'] = df.PdDistrict.apply(lambda item: PD_map[item])\n",
    "    \n",
    "    addresses = sorted(df[\"Address\"].unique())\n",
    "    categories = sorted(df[\"NewCategory\"].unique())\n",
    "    C_counts = df.groupby([\"NewCategory\"]).size()\n",
    "    logoddsPA = dict((df.groupby('Address').size()/len(df)).apply(odds))\n",
    "    default_logodds = np.log(C_counts/len(df))-np.log(1.0-C_counts/float(len(df)))\n",
    "    oddUpdate = pd.Series(((df.groupby(['Address','NewCategory']).size()/df.groupby(['Address']).size()).apply(odds)))\n",
    "    logodds = {k:update_odds(k, default_logodds, oddUpdate) for k in addresses}\n",
    "    address_features=df[\"Address\"].apply(lambda x: logodds[x])\n",
    "    address_features.columns=[\"logodds\"+str(x) for x in range(len(address_features.columns))]\n",
    "\n",
    "    to_learn = pd.concat([to_learn, address_features], axis=1)\n",
    "    \n",
    "    col = [col for col in to_learn.columns if col not in ['Category']]\n",
    "    category, features = to_learn.Category.as_matrix(), to_learn[col].as_matrix()\n",
    "\n",
    "    new_PCA=PCA(n_components=7)\n",
    "    features = new_PCA.fit_transform(features)\n",
    "    np.savetxt('to_learn_' + str(csv_num) + '.csv', np.hstack((np.reshape(category, (category.size, 1)), features)))\n",
    "    \n",
    "    arr = []\n",
    "    print(file_name)\n",
    "    for i in range(10):\n",
    "        features_train, features_test, category_train, category_test = train_test_split(features, category, test_size=0.2)\n",
    "        model = RandomForestClassifier().fit(features_train, category_train)\n",
    "        prediction = model.predict(features_test)\n",
    "        acc = accuracy_score(category_test, prediction)\n",
    "        arr.append(acc)\n",
    "        print acc\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "The experiment itself which iterates through each year and calculate 5-fold cross validation accuracy for 10 times. After the experiment terminates, we would have a .txt file recording the experiment results, as well as year-separated .csv files for machine learing with spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003.csv\n",
      "0.580745551722\n",
      "0.585143089101\n",
      "0.581692713619\n",
      "0.58676679521\n",
      "0.582030985725\n",
      "0.578783573507\n",
      "0.580948514985\n",
      "0.574250727285\n",
      "0.585143089101\n",
      "0.578039374873\n",
      "2004.csv\n",
      "0.591828396323\n",
      "0.58100102145\n",
      "0.5873340143\n",
      "0.592645556691\n",
      "0.589036431733\n",
      "0.583520599251\n",
      "0.592168879809\n",
      "0.586993530814\n",
      "0.592986040177\n",
      "0.586040177051\n",
      "2005.csv\n",
      "0.59148064425\n",
      "0.587100875954\n",
      "0.586041254592\n",
      "0.59352924555\n",
      "0.596708109635\n",
      "0.589855891495\n",
      "0.592328341339\n",
      "0.591551285674\n",
      "0.588443063012\n",
      "0.59063294716\n",
      "2006.csv\n",
      "0.535331140037\n",
      "0.532684880561\n",
      "0.53726219425\n",
      "0.537476755829\n",
      "0.530896867401\n",
      "0.529466456873\n",
      "0.541839507939\n",
      "0.539264768989\n",
      "0.535259619511\n",
      "0.529824059505\n",
      "2007.csv\n",
      "0.554951113725\n",
      "0.550393295597\n",
      "0.553407336617\n",
      "0.55620083805\n",
      "0.546276556642\n",
      "0.559214879071\n",
      "0.547452767772\n",
      "0.541057119753\n",
      "0.545541424686\n",
      "0.555759758877\n",
      "2008.csv\n",
      "0.560313501959\n",
      "0.557748485928\n",
      "0.560313501959\n",
      "0.553259707873\n",
      "0.55133594585\n",
      "0.560384752405\n",
      "0.544068400428\n",
      "0.561738510866\n",
      "0.551763448522\n",
      "0.549697185607\n",
      "2009.csv\n",
      "0.559637681159\n",
      "0.554927536232\n",
      "0.55115942029\n",
      "0.566376811594\n",
      "0.555362318841\n",
      "0.553260869565\n",
      "0.560072463768\n",
      "0.55615942029\n",
      "0.556811594203\n",
      "0.557753623188\n",
      "2010.csv\n",
      "0.565857690285\n",
      "0.571868660305\n",
      "0.573747088436\n",
      "0.576602299196\n",
      "0.570816740552\n",
      "0.569313998046\n",
      "0.570666466301\n",
      "0.570891877677\n",
      "0.566759335788\n",
      "0.57479900819\n",
      "2011.csv\n",
      "0.579705794056\n",
      "0.576778745122\n",
      "0.575502851996\n",
      "0.585334734314\n",
      "0.581732212549\n",
      "0.580156109276\n",
      "0.572800960672\n",
      "0.579105373762\n",
      "0.577829480636\n",
      "0.581582107475\n",
      "2012.csv\n",
      "0.589112706489\n",
      "0.589252108455\n",
      "0.591831044818\n",
      "0.590227922214\n",
      "0.580190980693\n",
      "0.588067191747\n",
      "0.586045863247\n",
      "0.58513975047\n",
      "0.587161078971\n",
      "0.591552240887\n",
      "2013.csv\n",
      "0.620817352202\n",
      "0.612948022748\n",
      "0.624256050787\n",
      "0.616386721333\n",
      "0.623793149054\n",
      "0.619362518185\n",
      "0.618965745272\n",
      "0.61843671472\n",
      "0.615394789049\n",
      "0.623727020235\n",
      "2014.csv\n",
      "0.625183897285\n",
      "0.625919486425\n",
      "0.632472916945\n",
      "0.629329945165\n",
      "0.623177745085\n",
      "0.624782666845\n",
      "0.624247692925\n",
      "0.625183897285\n",
      "0.624582051625\n",
      "0.626989434265\n"
     ]
    }
   ],
   "source": [
    "arr = []\n",
    "for i in range(2003, 2015):\n",
    "    arr.append(experiment(str(i) + '.csv', i))\n",
    "np.savetxt('sklearn_results.txt', np.array(arr), delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
